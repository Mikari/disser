---
date: January 2017
title: xxx
author:
  - name: Egor Khairullin
    affilation: MIPT, YSDA
    email: mikari.san@gmail.com

abstract: Результатами многих алгоритмов машинного обучения является довольно сложная и медленно применяющаяся модель. Однако такие модели хочется применять в условиях ограниченных ресурсов и пропускать через них как можно больше событий.
Данная статья рассматривает способ размена качества модели на скорость её применения для алгоритма Matrixnet, основанного на бустинге деревьями. Идея заключается в объединении двух подходв: обучение меньшего количества деревьев и объединение деревьев в огромные деревья-кубы. <...>

---

# 1. Введение.
Сегодня машинное обучение используется очень во многих сферах и на различных устройствах. Результатом многих алгоритмов являются довольно медленно применяющиеся модели. Например, хорошая модель при использовании бустинга деревьями содержит обычно тысячи обычных деревьев. Это выливается в довольно медленное применение таких моделей, несмотря на технические оптимизации (использование векторных вычислений). А попытки снизить количество деревьев обычно приводят к очень сильной потери качества. В даннcой работе рассматривается способ ускорить применение модели за счет лишь небольшой потери в качестве. За основу был взят алгоритм машинного обучения Matrixnet. В его основе лежит Gradient Boosting of Oblivious Decision Tree.

# 2. Гиперкуб
Пусть n - количество факторов, T - деревья, полученные в результате обучения некоторого алгоритма. Для удобства будем считать, что значения всех факторов лежат в [0;1].
Пусть B_i - набор разрезов для i'ого фактора, которые использовались внутри набора T. Тогда с помощью плоскостей X_i = B_ij для всех i,j мы можем разбить гиперкуб [0;1] на небольшие гиперпараллелепипеды. Очевидно, что внутри каждого кусочка предсказание модели будет постоянным.
По сути, наша модель позволяет посчитать значение в любой точке нашего гиперкуба. Однако, если мы предпосчитаем значение в каждом кусочке, то мы ускорим применение нашей модели, сведя её лишь к определению нужного кусочка. Асимптотическая сложность применения модели из деревьев - O(ht), где h - высота деревьев, t - общее количество деревьев. Асимптотическая сложность предпосчитанная сложность гиперкуба же - O(n * log(b)), где n - число факторов, а b - количество разрезов одного фактора (такая сложность достигается с помощью использования бинарного поиска).
Однако, для хранения такого гиперкуба нужно порядка O(b^n) памяти, что может быть слишком большим. Поэтому использовать предпосчитанный гиперкуб можно лишь при небольших b и n.
Рассмотрим случае при малых n. Получить небольшое b мы можем двумя основными способами: ограничить число разрезов для каждого фактора - тогда мы сможем обучить матрикснет с t=5000, либо сильно ограничить число деревьев - тогда общее количество действительно используемых разрезов будет ровно h * t без учета повторных использований одних и тех же разрезов.
В работе (ссылка на статью про матрикснет для церна) использовался первый способ: был обучен и превращен в "гиперкуб" матрикснет с небольшим количеством разрезов.
Продолжением и обобщением этих двух способов является получение сразу нескольких гиперкубов - это, конечно, будет медленнее, чем один гиперкуб, но, с другой стороны, потенциально позволит хранить в памяти более сложную модель, так как при том же качестве несколько гиперкубов занимают меньше памяти, чем один.

# 3. Объединение деревьев в несколько гиперкубов.
Допустим, у нас есть N деревьев и мы хотим объединить их в M гиперкубов. При M=1 задача тривиальна и рассматривалась выше. Рассмотрим M > 1. Метрикой оптимальности разбиения будем считать занимаемую память.
<Надо продумать разумное математическое обозначение и сформулировать оптимальность разбиения через формулы.>.

Для решения такой задачи можно использовать несколько эвристических метдов:
1) Вначале у нас N гиперкубов из 1 дерева. На каждой итерации находим и сливаем два наиболее схожих гиперкуба (схожих - то есть итоговая метрика вырастет слабее всего)
2) Вначале у нас M гиперкубов и в каждом одно произвольно выбранное дерево. Выбираем любое из оставшихся деревьев и добавляем в наиболее схожий с ним гиперкуб
3) Аналогично 2-ому, но выбираем самое лучшее дерево.
Эти методы, очевидно, не дают гарантированно лучшего варианта, но позволяют получить вариант, который лучше случайного разбиения. Мы в итоге брали самый лучший из вариант из всех трех.

Общая схема обучения для получения N гиперкубов выглядит таким образом:
1) Обучаем T деревьев несколько раз с помощью матрикснета с разной регуляризацией
2) Для каждого префикса каждого набора из T деревьев пытаемся построить оптимальное разбиение на N деревьев с помощью трех эвристик
3) В результате получаем большое количество наборов гиперкубов, каждый описывается парой (размер, качество). Теперь можно выкинуть все те наборы, для которых найдется набор с меньшим размером, но большим качеством

# 4. Применение на практике.
На графике 1 представлены результаты.
1) Baseline - обученные стандартным образом с помощью матрикснета 5000 деревьев.
2) Rude матрикснет, обученный аналогичным образом, но с очень небольшим количеством сплитов, позволяющее его легко превратить в гиперкуб.
Оба этих матрикснета нарисованы в виде линий (реальный размер Baseline - несколько мегабайт, а размер Rude - около 120мегабайт).
3) Несколько гиперкубов, полученных с помощью метода, описанных в пункте 3 при T=100.
4) Rude + несколько гиперкубов.

![качество против размера](quality_vs_size.png)

По графику видно несколько моментов. Во-первых, Rude гиперкуб при разумных размерах модели оказывается значительно лучше, чем набор 3. А добавление к Rude ещё нескольких гиперкубов - довольно значительно его бустит.
