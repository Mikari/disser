<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>xxx</title>
  <style type="text/css">code{white-space: pre;}</style>
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="/Users/mikari/.pandoc/marked/kultiad-serif.css">
</head>
<body>
<header>
<h3 class="date">January 2017</h3>

<h1 class="title">xxx</h1>


<h2 class="author">Egor Khairullin</h2>
<p class="affilation"><em></em></p>

</header>


<p class="small"><strong>Abstract: </strong><em>The result of many machine learning algorithms are complex and slowly applied models. And further growth in the quality of the such models usually leads to a deterioration in the application times. However, such high quality models are desirable to be used in the conditions of limited resources (memory or cpu time).</em></p>



<h1 id="введение.">1. Введение.</h1>
<p>Сегодня машинное обучение (МО) используется в различных сферах: от поиска информации в интернете и распознавания речи до оптимизации состава стали и поиска новой физики. Практически во всех случаях для решения поставленной задачи очень ограниченны ресурсы (память, процессор, сеть и т.д.). Для многих алгоритмов МО характерно, что повышение качества модели связанно с повышением уровня потребления ресурсов. Например, при использовании алгоритмов бустинга над деревьями, повышение числа деревьев с десятков до тысяч обычно ведет к значительному росту качества, одновременно увеличивая нагрузку на память и процессор при использовании такой модели. Повышенные требования к аппаратным ресурсам более точных моделей обычно решают за счет различных технических оптимизаций хранения и применения полученной модели: например, с помощью векторных вычислений. Другим подходом является модификация самого процесса обучения, позволяющее получать модели с определенными свойствами, которые позволяют значительно ускорять их применение, не сохраняя необходимый уровень качества. В данной работе мы рассмотрим модификацию процесса обучения предсказательных моделей алгоритмом MatrixNet <span class="citation">(Gulin, Kuralenok, and Pavlov 2011)</span>, которая позволит ускорить использование модели с сохранением необходимого уровня качества.</p>
<h1 id="matrixnet">2. MatrixNet</h1>
<p>MatrixNet - является реализацией градиентного бустинга над деревьями, в котором используются так называемые “невнимательные” деревья решений (Oblivious Decision Tree). В данном алгоритме значения каждого признака <span class="math inline"><em>i</em></span> разбиваются на корзинки с помощью границ <span class="math inline"><em>B</em><sub><em>i</em></sub></span>. Сами границы определяются заранее с помощью различных статистик. А исходный вектор с вещественными значениями заменяется на вектор с бинарными значениями <span class="math inline">0</span> и <span class="math inline">1</span> (<span class="math inline"><em>f</em><em>a</em><em>l</em><em>s</em><em>e</em></span> и <span class="math inline"><em>t</em><em>r</em><em>u</em><em>e</em></span>): значение <span class="math inline"><em>i</em></span>-ого признака <span class="math inline"><em>f</em><sub><em>i</em></sub></span> заменяется на бинарный вектор <span class="math inline"><em>g</em><sub><em>i</em></sub></span>, где <span class="math inline"><em>g</em><sub><em>i</em><em>j</em></sub> = <em>f</em><sub><em>i</em></sub> &gt; <em>B</em><sub><em>i</em><em>j</em></sub></span>, после чего все <span class="math inline"><em>g</em><sub><em>i</em></sub></span> конкатенируются в один большой вектор. И в получаемых деревьях вместо сравнения некоторого вещественного признака с некоторым вещественным порогом фактически будет находится сравнение с <span class="math inline">0</span> и <span class="math inline">1</span>. В матрикснете можно управлять количеством корзинок, передавая соответствующий параметр при обучении. Другой достаточно важный параметр - степень регуляризации. Чем он меньше - тем с меньшим вкладом берется каждое новое дерево. Однако, это приходится компенсировать большим количеством деревьев. Данный параметр можно подбирать с помощью кроссвалидации, фиксируя при этом общее количество деревьев.</p>
<p>За основу был взят датасет и модель из <span class="citation">(Likhomanenko et al. 2015)</span>. Количество признаков в датасете ~10, модель была обучена на 5000 деревьях с признаком разбиений 64. Очевидное решение исследуемой проблемы: сильно завысить скорость обучения и получить модель с очень небольшим количеством деревьев (меньше 10) хотя и позволяет достичь приемлимой скорости применения, приводит к сильной потери качества. Модель из данной работы была взята как baseline - именно с ней мы будем сравнивать результаты.</p>
<h1 id="гиперкуб">3. Гиперкуб</h1>
<p>Одним из самых простых способов ускорения является предварительный расчет. Вектор признаков, получаемый после бинаризации, принимает ограниченное число значений. Поэтому мы можем предпосчитать ответ для всех таких значений.</p>
<p>Пусть <span class="math inline"><em>n</em></span> - количество признаков. Рассмотрим некоторую обученную с помощью некоторого алгоритма модель <span class="math inline"><em>Z</em></span>, содержащую множество деревьев <span class="math inline"><em>T</em></span>. Для удобства будем считать, что значения всех признаков лежат в отрезке <span class="math inline">[0, 1]</span>. Пусть <span class="math inline"><em>B</em><sub><em>i</em></sub></span> - набор границ корзинок для <span class="math inline"><em>i</em></span>’ого признака, которые использовались внутри набора <span class="math inline"><em>T</em></span>. Тогда с помощью плоскостей <span class="math inline"><em>X</em><sub><em>i</em></sub></span> = <span class="math inline"><em>B</em><sub><em>i</em><em>j</em></sub></span> для всех <span class="math inline"><em>i</em>, <em>j</em></span> мы можем разбить гиперкуб <span class="math inline">[0; 1]</span> на небольшие гиперпараллелепипеды. Очевидно, что внутри каждого гиперпараллелепипеда предсказание модели будет постоянным. По сути, модель <span class="math inline"><em>Z</em></span> позволяет посчитать значение в любой точке нашего гиперкуба. Однако, если мы предпосчитаем значение в каждом кусочке, то мы ускорим применение нашей модели, сведя её лишь к определению нужного кусочка. Асимптотическая сложность применения модели из деревьев - <span class="math inline"><em>O</em>(<em>h</em> ⋅ <em>t</em>)</span>, где <span class="math inline"><em>h</em></span> - высота деревьев, <span class="math inline"><em>t</em></span> - общее количество деревьев. Асимптотическая сложность предпосчитанная сложность гиперкуба же - <span class="math inline"><em>O</em>(<em>n</em> ⋅ <em>l</em><em>o</em><em>g</em>(<em>b</em>))</span>, где <span class="math inline"><em>n</em></span> - число признаков, а <span class="math inline"><em>b</em></span> - количество разрезов одного признака (такая сложность достигается с помощью использования бинарного поиска). При малом количестве признаков такой гиперкуб может применяться значительно быстрее. Однако, для хранения такого гиперкуба нужно порядка <span class="math inline"><em>O</em>(<em>b</em><sup><em>n</em></sup>)</span> памяти, что может быть слишком большим. Поэтому использовать предпосчитанный гиперкуб можно лишь при небольших <span class="math inline"><em>b</em></span> и <span class="math inline"><em>n</em></span>. Рассмотрим случае при малых n. Получить небольшое <span class="math inline"><em>b</em></span> мы можем двумя основными способами: ограничить число разрезов для каждого признака - тогда мы сможем обучить MatrixNet с <span class="math inline"><em>t</em> = 5000</span>, либо сильно ограничить число деревьев - тогда общее количество действительно используемых разрезов будет ровно <span class="math inline"><em>h</em> ⋅ <em>t</em></span> без учета повторных использований одних и тех же разрезов. В работе <span class="citation">(Likhomanenko et al. 2015)</span> использовался первый способ: был обучен и превращен в “гиперкуб” MatrixNet с небольшим количеством разрезов. Потребление памяти у одного гиперкуба может быть чрезвычайно огромным при условии достижения приемлимого качества, поэтому естественным продолжением является построение несколько гиперкубов.</p>
<h1 id="задача-многокритериальной-оптимизации">4. Задача многокритериальной оптимизации</h1>
<p>Таким образом, целевыми метриками является <span class="math inline"><em>Q</em></span> (качество модели), <span class="math inline"><em>S</em></span> (размер) и <span class="math inline"><em>M</em></span> (количество гиперкубов). Конечно, нас скорее интересует время применения такой модели, однако можно считать, что время работы прямо пропорционально количеству гиперкубов. Входной же вектор должен будет описывать алгоритм получения деревьев и алгоритм объединения их в кубы. Обозначим его <span class="math inline"><em>V</em></span>. Тогда мы решаем следующую задачу:</p>
<p>min{<span class="math inline"><em>Q</em></span>(<span class="math inline"><em>V</em></span>), <span class="math inline"><em>S</em></span>(<span class="math inline"><em>V</em></span>), <span class="math inline"><em>M</em></span>(<span class="math inline"><em>V</em></span>)} по всем возможным <span class="math inline"><em>V</em></span>.</p>
<p>Прежде чем мы приступим к рассмотрению содержимого вектора <span class="math inline"><em>X</em></span>, необходимо отметить, что для практического применения данную задачу можно решать с помощью метода ограничений: обычно на практике есть довольно строгие ограничения на размер модели и доступное время применения.</p>
<h1 id="алгоритм-объединения-набора-деревьев-в-несколько-гиперкубов.">5. Алгоритм объединения набора деревьев в несколько гиперкубов.</h1>
<p>Допустим, у нас есть <span class="math inline"><em>N</em></span> деревьев и мы хотим объединить их в <span class="math inline"><em>M</em></span> гиперкубов. При <span class="math inline"><em>M</em> = 1</span> задача тривиальна и рассматривалась выше. Рассмотрим <span class="math inline"><em>M</em> &gt; 1</span>. Метрикой оптимальности разбиения будем считать занимаемую память.</p>
<p><Надо продумать разумное математическое обозначение и сформулировать оптимальность разбиения через формулы.> В общем случае, это непростая задача и она требует отдельного изучения. Мы же использовали несколько эвристических методов: 1) Вначале у нас <span class="math inline"><em>N</em></span> гиперкубов из <span class="math inline">1</span> дерева. На каждой итерации находим и сливаем два наиболее схожих гиперкуба (схожих - то есть итоговая метрика вырастет слабее всего) 2) Вначале у нас <span class="math inline"><em>M</em></span> гиперкубов и в каждом одно произвольно выбранное дерево. Выбираем любое из оставшихся деревьев и добавляем в наиболее схожий с ним гиперкуб 3) Аналогично 2-ому, но выбираем самое лучшее дерево. Эти методы, очевидно, не дают гарантированно лучшего варианта, но позволяют получить вариант, который лучше случайного разбиения. Мы в итоге брали самый лучший из вариант из всех трех.</p>
<h1 id="алгоритм-обучения-подходящих-деревьев.">6. Алгоритм обучения подходящих деревьев.</h1>
<p>У MatrixNet есть несколько ключевых входных параметров: 1) Способ бинаризации факторов 2) Целевое число деревьев (<span class="math inline"><em>T</em></span>) 3) Степень регуляризации (<span class="math inline"><em>W</em></span>) Однако, мы не стали оптимизировать по всем возможным способам разбиения, перебирая лишь параметр фактор бинаризации <span class="math inline"><em>X</em></span> у встроенного в MatrixNet алгоритма. В целом, вместо MatrixNet можно использовать любой другой алгоритм обучения деревьев.</p>
<p>Опять же, мы уменьшаем исследуемое пространство следующим соображением: размер куба пропорционален произведению количества используемых бинов у каждой фичи. А значит, если мы хотим получать малые кубы, то итоговое множество деревьев должно использовать малое количество бинов. Тут есть два ключевых подхода: 1) Фиксируем очень грубое разбиение с малым количеством кубов и обучаем сколь угодно большое количество деревьев. 2) Разбиение делаем достаточно детальное, но при этом значительно уменьшаем количество обученных деревьев, тем самым снижая количество реально используемых бинов.</p>
<p>При внедрении на LHCb модели, обученной Matrixnet’ом мы использовали первый подход. В этой работе мы пробуем применить второй подход и скомбинировать его с первым. Так у нас получились такие способы обучения деревьев: 1) “Rude”: очень небольшой <span class="math inline"><em>X</em></span>, большое <span class="math inline"><em>T</em></span>. На выходе получается из-за небольшого <span class="math inline"><em>X</em></span> все деревья можно объединить в большой гиперкуб. 2) “Small”: большой <span class="math inline"><em>X</em></span>, небольшое <span class="math inline"><em>T</em></span>. Из-за большого <span class="math inline"><em>X</em></span> деревья получаются очень разные и значительно хуже склеиваются, из-за чего приходится сильно ограничивать число деревьев, используемых для склейки в гиперкубы. 3) Комбинированный: обучаем Small по бейзлайну Rude и берем все получившиеся гиперкубы в одну модель.</p>
<h1 id="решение-оптимизационной-задачи">7. Решение оптимизационной задачи</h1>
<p>Как было сказано выше, мы решали оптимизационную задачу с ограничениями, фиксируя разные значения <span class="math inline"><em>S</em></span> и <span class="math inline"><em>M</em></span> (при этом глобально <span class="math inline"><em>S</em></span> ограничен 1gb, а <span class="math inline"><em>M</em></span> &lt; 5). Для каждого случая мы сравнивали между собой все три способа получения деревьев, а так же сравнивали с эталонным решением: оптимальным образом обученный MatrixNet без каких либо ограничений.</p>
<ol style="list-style-type: decimal">
<li>В качестве эталонного решения был взят MatrixNet с <span class="math inline"><em>T</em> = 5000</span>, <span class="math inline"><em>X</em> = 64</span> и оптимальным <span class="math inline"><em>W</em></span>.</li>
<li>Rude - мы сохранили <span class="math inline"><em>T</em> = 5000</span> от эталона, однако взяли <span class="math inline"><em>X</em> = 5</span> - это максимальное <span class="math inline"><em>X</em></span>, при котором получаемый гиперкуб укладывается в 1GB.</li>
<li>Small - <span class="math inline"><em>W</em></span> и <span class="math inline"><em>T</em></span> мы перебирали поиском по сетке. Фактически количество деревьев не превышало 100. При большем значении либо получалось слишком много гиперкубов, либо слишком большой размер. Полученные деревья мы разбивали в кубы, опираясь на текущее значение <span class="math inline"><em>M</em></span>. В качестве фактора разбиения мы брали <span class="math inline"><em>X</em> = 64</span> (как у эталона).</li>
<li>Комбинированный подход объединяет пункты 2 и 3.</li>
</ol>
<h1 id="обсуждение.">8. Обсуждение.</h1>
<p>На графике 1 представлены результаты. По оси x указано ограничение на суммарный размер модели S, а по оси y - итоговое качество модели Q.</p>
<ol style="list-style-type: decimal">
<li>Baseline - обычный MatrixNet с <span class="math inline"><em>T</em> = 5000</span>, <span class="math inline"><em>X</em> = 64</span>, изображен линией для удобства, хотя потребление памяти у обычной модели очень мало</li>
<li>Rude - при <span class="math inline"><em>T</em> = 5000</span>, <span class="math inline"><em>X</em> = 5</span></li>
<li>Small - при <span class="math inline"><em>T</em> &lt; 100</span>, <span class="math inline"><em>X</em> = 64</span> при различных <span class="math inline"><em>M</em></span>.</li>
<li>Rude + Small По графику видно, что способ 4 дает наибольшее итогое качество. Таким образом итого можно порекомендовать в качестве первого гиперкуба брать rude схему, а при недостаточном качестве добавлять Small гиперкубы, полученные из MatrixNet, обученного на небольшом количестве деревьев.</li>
</ol>
<div class="figure">
<img src="quality_vs_size.png" alt="График 1. Качество против размера" />
<p class="caption">График 1. Качество против размера</p>
</div>
<p>Стоит отметить, что при малом X MatrixNet строит не самые лучшие разбиения признаков. Поэтому в дальнейших исследованиях необходимо рассмотреть способы нахождения более оптимальных разбиений, чем получается с помощью простых статистических методов.</p>
<h1 id="заключение.">9. Заключение.</h1>
<p>(<em>TODO</em>: написать)</p>
<h1 id="ссылки" class="unnumbered">Ссылки</h1>
<div id="refs" class="references">
<div id="ref-gulin2011winning">
<p>Gulin, Andrey, Igor Kuralenok, and Dmitry Pavlov. 2011. “Winning the Transfer Learning Track of Yahoo!’s Learning to Rank Challenge with Yetirank.” In <em>Yahoo! Learning to Rank Challenge</em>, 63–76.</p>
</div>
<div id="ref-likhomanenko2015lhcb">
<p>Likhomanenko, Tatiana, Philip Ilten, Egor Khairullin, Alex Rogozhnikov, Andrey Ustyuzhanin, and Michael Williams. 2015. “LHCb Topological Trigger Reoptimization.” In <em>Journal of Physics: Conference Series</em>, IOP Publishing, 082025.</p>
</div>
</div>
</body>
</html>
