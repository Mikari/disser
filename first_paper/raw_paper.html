<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>xxx</title>
  <style type="text/css">code{white-space: pre;}</style>
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="/Users/anaderi/.pandoc/marked/kultiad-serif.css">
</head>
<body>
<header>
<h3 class="date">January 2017</h3>

<h1 class="title">xxx</h1>


<h2 class="author">Egor Khairullin</h2>
<p class="affilation"><em></em></p>

</header>


<p class="small"><strong>Abstract: </strong><em>Результатами многих алгоритмов машинного обучения является довольно сложная и медленно применяющаяся модель. Однако такие модели хочется применять в условиях ограниченных ресурсов и пропускать через них как можно больше событий.</em></p>



<h1 id="введение.">1. Введение.</h1>
<p>Сегодня машинное обучение используется очень во многих сферах и на различных устройствах. Результатом многих алгоритмов являются довольно медленно применяющиеся модели. Например, хорошая модель при использовании бустинга деревьями содержит обычно тысячи обычных деревьев. Это выливается в довольно медленное применение таких моделей, несмотря на технические оптимизации (использование векторных вычислений). А попытки снизить количество деревьев обычно приводят к очень сильной потери качества. В даннcой работе рассматривается способ ускорить применение модели за счет лишь небольшой потери в качестве. За основу был взят алгоритм машинного обучения Matrixnet. В его основе лежит Gradient Boosting of Oblivious Decision Tree.</p>
<h1 id="гиперкуб">2. Гиперкуб</h1>
<p>Пусть n - количество факторов, <span class="math inline"><em>T</em></span> - деревья, полученные в результате обучения некоторого алгоритма. Для удобства будем считать, что значения всех факторов лежат в <span class="math inline">[0; 1]</span>. Пусть <span class="math inline"><em>B</em><sub><em>i</em></sub></span> - набор разрезов для <span class="math inline"><em>i</em></span>’ого фактора, которые использовались внутри набора T. Тогда с помощью плоскостей <span class="math inline"><em>X</em><sub><em>i</em></sub> = <em>B</em><sub><em>i</em><em>j</em></sub></span> для всех <span class="math inline"><em>i</em>, <em>j</em></span> мы можем разбить гиперкуб <span class="math inline">[0; 1]</span> на небольшие гиперпараллелепипеды. Очевидно, что внутри каждого кусочка предсказание модели будет постоянным. По сути, наша модель позволяет посчитать значение в любой точке нашего гиперкуба. Однако, если мы предпосчитаем значение в каждом кусочке, то мы ускорим применение нашей модели, сведя её лишь к определению нужного кусочка. Асимптотическая сложность применения модели из деревьев - <span class="math inline"><em>O</em>(<em>h</em> ⋅ <em>t</em>)</span>, где <span class="math inline"><em>h</em></span> - высота деревьев, <span class="math inline"><em>t</em></span> - общее количество деревьев. Асимптотическая сложность предпосчитанная сложность гиперкуба же - <span class="math inline"><em>O</em>(<em>n</em> ⋅ <em>l</em><em>o</em><em>g</em>(<em>b</em>))</span>, где <span class="math inline"><em>n</em></span> - число факторов, а <span class="math inline"><em>b</em></span> - количество разрезов одного фактора (такая сложность достигается с помощью использования бинарного поиска). Однако, для хранения такого гиперкуба нужно порядка <span class="math inline"><em>O</em>(<em>b</em><sup><em>n</em></sup>)</span> памяти, что может быть слишком большим. Поэтому использовать предпосчитанный гиперкуб можно лишь при небольших <span class="math inline"><em>b</em></span> и <span class="math inline"><em>n</em></span>. Рассмотрим случае при малых n. Получить небольшое <span class="math inline"><em>b</em></span> мы можем двумя основными способами: ограничить число разрезов для каждого фактора - тогда мы сможем обучить матрикснет с <span class="math inline"><em>t</em> = 5000</span>, либо сильно ограничить число деревьев - тогда общее количество действительно используемых разрезов будет ровно <span class="math inline"><em>h</em> ⋅ <em>t</em></span> без учета повторных использований одних и тех же разрезов. В работе (ссылка на статью про матрикснет для церна) использовался первый способ: был обучен и превращен в “гиперкуб” матрикснет с небольшим количеством разрезов. Продолжением и обобщением этих двух способов является получение сразу нескольких гиперкубов - это, конечно, будет медленнее, чем один гиперкуб, но, с другой стороны, потенциально позволит хранить в памяти более сложную модель, так как при том же качестве несколько гиперкубов занимают меньше памяти, чем один.</p>
<h1 id="объединение-деревьев-в-несколько-гиперкубов.">3. Объединение деревьев в несколько гиперкубов.</h1>
<p>Допустим, у нас есть <span class="math inline"><em>N</em></span> деревьев и мы хотим объединить их в <span class="math inline"><em>M</em></span> гиперкубов. При <span class="math inline"><em>M</em> = 1</span> задача тривиальна и рассматривалась выше. Рассмотрим случай <span class="math inline"><em>M</em> &gt; 1</span>. Метрикой оптимальности разбиения будем считать занимаемую память. <Надо продумать разумное математическое обозначение и сформулировать оптимальность разбиения через формулы.>.</p>
<p>Для решения такой задачи можно использовать несколько эвристических метдов:</p>
<ol style="list-style-type: decimal">
<li>Вначале у нас <span class="math inline"><em>N</em></span> гиперкубов из <span class="math inline">1</span> дерева. На каждой итерации находим и сливаем два наиболее схожих гиперкуба (схожих - то есть итоговая метрика вырастет слабее всего)</li>
<li>Вначале у нас <span class="math inline"><em>M</em></span> гиперкубов и в каждом одно произвольно выбранное дерево. Выбираем любое из оставшихся деревьев и добавляем в наиболее схожий с ним гиперкуб</li>
<li>Аналогично 2-ому, но выбираем самое лучшее дерево.</li>
</ol>
<p>Эти методы, очевидно, не дают гарантированно лучшего варианта, но позволяют получить вариант, который лучше случайного разбиения. Мы в итоге брали самый лучший из вариант из всех трех.</p>
<p>Общая схема обучения для получения <span class="math inline"><em>N</em></span> гиперкубов выглядит таким образом:</p>
<ol style="list-style-type: decimal">
<li>Обучаем <span class="math inline"><em>T</em></span> деревьев несколько раз с помощью матрикснета с разной регуляризацией</li>
<li>Для каждого префикса каждого набора из <span class="math inline"><em>T</em></span> деревьев пытаемся построить оптимальное разбиение на <span class="math inline"><em>N</em></span> деревьев с помощью трех эвристик</li>
<li>В результате получаем большое количество наборов гиперкубов, каждый описывается парой (размер, качество). Теперь можно выкинуть все те наборы, для которых найдется набор с меньшим размером, но большим качеством</li>
</ol>
<h1 id="применение-на-практике.">4. Применение на практике.</h1>
<p>На графике 1 представлены результаты.</p>
<ol style="list-style-type: decimal">
<li>Baseline - обученные стандартным образом с помощью матрикснета <span class="math inline">5000</span> деревьев.</li>
<li>Rude матрикснет, обученный аналогичным образом, но с очень небольшим количеством сплитов, позволяющее его легко превратить в гиперкуб. Оба этих матрикснета нарисованы в виде линий (реальный размер Baseline - несколько мегабайт, а размер Rude - около 120 мегабайт).</li>
<li>Несколько гиперкубов, полученных с помощью метода, описанных в пункте 3 при <span class="math inline"><em>T</em> = 100</span>.</li>
<li>Rude + несколько гиперкубов.</li>
</ol>
<div class="figure">
<img src="quality_vs_size.png" alt="качество против размера" />
<p class="caption">качество против размера</p>
</div>
<p>По графику видно несколько моментов. Во-первых, Rude гиперкуб при разумных размерах модели оказывается значительно лучше, чем набор 3. А добавление к Rude ещё нескольких гиперкубов - довольно значительно его бустит.</p>
</body>
</html>
