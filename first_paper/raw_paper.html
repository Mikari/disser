<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>xxx</title>
  <style type="text/css">code{white-space: pre;}</style>
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="/Users/mikari/.pandoc/marked/kultiad-serif.css">
</head>
<body>
<header>
<h3 class="date">January 2017</h3>

<h1 class="title">xxx</h1>


<h2 class="author">Egor Khairullin</h2>
<p class="affilation"><em></em></p>

</header>


<p class="small"><strong>Abstract: </strong><em>Результатами многих алгоритмов машинного обучения является довольно сложная и медленно применяющаяся модель. Однако такие модели хочется применять в условиях ограниченных ресурсов и пропускать через них как можно больше событий.</em></p>



<h1 id="введение.">1. Введение.</h1>
<p>Сегодня машинное обучение используется в различных сферах: от поиска информации в интернете и распознавания речи до оптимизации состава стали и поиска новой физики. Практически во всех случаях для решения поставленной задачи очень ограниченны ресурсы (память, цпу, сеть и т.д.). Для многих алгоритмов МО же характерно, что повышение качества модели обычно связанно с повышением потребления ресурсов. Например, при использовании бустинга деревьев повышение числа деревьев с десятков до тысяч обычно ведет к значительному росту качества, одновременно увеличивая нагрузку на цпу при применении такой модели. Эту проблему часто решают с помощью различных технических оптимизаций хранения и применения полученной модели: например, с помощью векторных вычислений. Другим подходом является модификация самого процесса обучения, позволяющее получать модели с определенными свойствами, которые позволяют значительно ускорять применение, не теряя сильно в качестве модели. В данной работе мы рассмотрим способ модификации процесса обучения модели с помощью алгоритма МО Matrixnet компании Яндекс.</p>
<h1 id="matrixnet">2. Matrixnet</h1>
<p>Матрикснет - алгоритм МО, придуманный в Яндексе в 20XX году. Является реализацией градиентного бустинга деревьями, однако используются так называемые “невнимательные” деревья решений (Oblivious Decision Tree). Особенностью является регулируемое число разбиение для каждого фактора, определяемое на основании различных статистик до непосредственного обучения. Другой достаточно важный параметр - скорость обучения. Этот параметр позволяет выбирать между высоким качеством (большое число деревьев) и большой скоростью применения (маленькое число деревьев). За основу был взят датасет и модель из (статья про матрикснет для церна). Количество факторов в датасете ~10, бейзлайн-модель была обучена на 5000 деревьях с фактором разбиений 64. Очевидное решение исследуемой проблемы: сильно завысить скорость обучения и получить модель с очень небольшим количеством деревьев (меньше 10) хотя и позволяет достичь приемлимой скорости применения, приводит к сильной потери качества.</p>
<h1 id="гиперкуб">3. Гиперкуб</h1>
<p>Одним из самых простых способов ускорения является предпосчет. Пользуясь конечным множеством значений у каждого дерева (для Oblivious Decision Tree это не более, чем <span class="math inline">2<sup><em>h</em></sup></span> различных значений, где <span class="math inline"><em>h</em></span> - высота дерева), можно предпосчитать все значения для целого набора деревьев, тем самым ускорив вычисления в момент применения.</p>
<p>Пусть <span class="math inline"><em>n</em></span> - количество факторов, <span class="math inline"><em>T</em></span> - деревья, полученные в результате обучения некоторого алгоритма. Для удобства будем считать, что значения всех факторов лежат в <span class="math inline">[0; 1]</span>. Пусть <span class="math inline"><em>B</em><sub><em>i</em></sub></span> - набор разрезов для <span class="math inline"><em>i</em></span>’ого фактора, которые использовались внутри набора <span class="math inline"><em>T</em></span>. Тогда с помощью плоскостей <span class="math inline"><em>X</em><sub><em>i</em></sub></span> = <span class="math inline"><em>B</em><sub><em>i</em><em>j</em></sub></span> для всех <span class="math inline"><em>i</em>, <em>j</em></span> мы можем разбить гиперкуб <span class="math inline">[0; 1]</span> на небольшие гиперпараллелепипеды. Очевидно, что внутри каждого кусочка предсказание модели будет постоянным. По сути, наша модель позволяет посчитать значение в любой точке нашего гиперкуба. Однако, если мы предпосчитаем значение в каждом кусочке, то мы ускорим применение нашей модели, сведя её лишь к определению нужного кусочка. Асимптотическая сложность применения модели из деревьев - <span class="math inline"><em>O</em>(<em>h</em> ⋅ <em>t</em>)</span>, где <span class="math inline"><em>h</em></span> - высота деревьев, <span class="math inline"><em>t</em></span> - общее количество деревьев. Асимптотическая сложность предпосчитанная сложность гиперкуба же - <span class="math inline"><em>O</em>(<em>n</em> ⋅ <em>l</em><em>o</em><em>g</em>(<em>b</em>))</span>, где <span class="math inline"><em>n</em></span> - число факторов, а <span class="math inline"><em>b</em></span> - количество разрезов одного фактора (такая сложность достигается с помощью использования бинарного поиска). При малом количестве факторов такой гиперкуб может применяться значительно быстрее. Однако, для хранения такого гиперкуба нужно порядка <span class="math inline"><em>O</em>(<em>b</em><sup><em>n</em></sup>)</span> памяти, что может быть слишком большим. Поэтому использовать предпосчитанный гиперкуб можно лишь при небольших <span class="math inline"><em>b</em></span> и <span class="math inline"><em>n</em></span>. Рассмотрим случае при малых n. Получить небольшое <span class="math inline"><em>b</em></span> мы можем двумя основными способами: ограничить число разрезов для каждого фактора - тогда мы сможем обучить матрикснет с <span class="math inline"><em>t</em> = 5000</span>, либо сильно ограничить число деревьев - тогда общее количество действительно используемых разрезов будет ровно h * t без учета повторных использований одних и тех же разрезов. В работе (ссылка на статью про матрикснет для церна) использовался первый способ: был обучен и превращен в “гиперкуб” матрикснет с небольшим количеством разрезов. Потребление памяти у одного гиперкуба может быть чрезвычайно огромным при условии достижения приемлимого качества, поэтому естественным продолжением является построение несколько гиперкубов. Тут возникает две связанные задачи: 1) Как эффективно разбить N деревьев в M гиперкубов, минимизируя суммарный размер гиперкубов. 2) Как обучать деревья, позволяющие эффективно разбивать их в N деревьев.</p>
<h1 id="объединение-деревьев-в-несколько-гиперкубов.">3. Объединение деревьев в несколько гиперкубов.</h1>
<p>Допустим, у нас есть <span class="math inline"><em>N</em></span> деревьев и мы хотим объединить их в <span class="math inline"><em>M</em></span> гиперкубов. При <span class="math inline"><em>M</em> = 1</span> задача тривиальна и рассматривалась выше. Рассмотрим <span class="math inline"><em>M</em> &gt; 1</span>. Метрикой оптимальности разбиения будем считать занимаемую память.</p>
<p><Надо продумать разумное математическое обозначение и сформулировать оптимальность разбиения через формулы.></p>
<p>Для решения данной задачи ммы использовали несколько эвристических методов:</p>
<ol style="list-style-type: decimal">
<li>Вначале у нас <span class="math inline"><em>N</em></span> гиперкубов из <span class="math inline">1</span> дерева. На каждой итерации находим и сливаем два наиболее схожих гиперкуба (схожих - то есть итоговая метрика вырастет слабее всего)</li>
<li>Вначале у нас <span class="math inline"><em>M</em></span> гиперкубов и в каждом одно произвольно выбранное дерево. Выбираем любое из оставшихся деревьев и добавляем в наиболее схожий с ним гиперкуб</li>
<li>Аналогично 2-ому, но выбираем самое лучшее дерево. Эти методы, очевидно, не дают гарантированно лучшего варианта, но позволяют получить вариант, который лучше случайного разбиения. Мы в итоге брали самый лучший из вариант из всех трех.</li>
</ol>
<h1 id="обучение-подходящих-деревьев.">4. Обучение подходящих деревьев.</h1>
<p>Для получения гиперкубов можно использовать деревья, полученные простым запуском матрикснета. Важными параметрами тут будут:</p>
<ol style="list-style-type: decimal">
<li>Фактор разбиения фичей (<span class="math inline"><em>X</em></span>)</li>
<li>Число деревьев (<span class="math inline"><em>T</em></span>)</li>
<li>Скорость обучения (<span class="math inline"><em>W</em></span>)</li>
<li>Итоговое качество (<span class="math inline"><em>Q</em></span>)</li>
<li>Число гиперкубов (<span class="math inline"><em>M</em></span>)</li>
<li>Итоговый размер гиперкубов (<span class="math inline"><em>S</em></span>)</li>
</ol>
<p>Оптимизационная задача была поставлена таким образом: максимизируем <span class="math inline"><em>Q</em></span>, при <span class="math inline"><em>S</em> &lt; =<em>S</em><sub><em>T</em></sub>, <em>M</em> &lt; =<em>M</em><sub><em>T</em></sub></span>, где <span class="math inline"><em>M</em><sub><em>T</em></sub></span> и <span class="math inline"><em>S</em><sub><em>T</em></sub></span> определяются исходя из требуемой скорости применения модели и максимально доступной памяти.</p>
<p>Исследовалось несколько следующих частных способов: 1) “Rude”: очень небольшой <span class="math inline"><em>X</em></span>, большое <span class="math inline"><em>T</em></span>, <span class="math inline"><em>M</em> = 1</span>. На выходе получается из-за очень малого <span class="math inline"><em>X</em></span> один достаточно компактный гиперкуб. 2) “Small”: большой <span class="math inline"><em>X</em></span>, <span class="math inline"><em>T</em> &lt; 100</span>, <span class="math inline"><em>M</em> &lt; 10</span>. Из-за большого <span class="math inline"><em>X</em></span> деревья получаются очень разные и значительно хуже склеиваются, из-за чего приходится сильно ограничивать число деревьев, используемых для склейки в гиперкубы. 3) Аналогично 2, но взяв в качестве бейзлайна при обучение пункт 1.</p>
<p>Для поиска оптимальных решений мы использовали поиск по сетке с параметрами T и W. Для каждой пары строился оптимальный набор гиперкубов. Среди всех полученных наборов отбирался лучший по качеству, укладывающийся в потребление памяти.</p>
<h1 id="результаты.">5. Результаты.</h1>
<p>На графике 1 представлены результаты. По оси x указано ограничение на суммарный размер модели S, а по оси y - итоговое качество модели Q.</p>
<ol style="list-style-type: decimal">
<li>Baseline - обычный матрикснет с <span class="math inline"><em>T</em> = 5000</span>, <span class="math inline"><em>X</em> = 64</span>, нарисован линий, так как потребление памяти у обычного матрикстнеа очень мало.</li>
<li>Rude - при <span class="math inline"><em>T</em> = 5000</span>, <span class="math inline"><em>X</em> = 5</span></li>
<li>Small - при <span class="math inline"><em>T</em> &lt; 100</span>, <span class="math inline"><em>X</em> = 64</span> при различных <span class="math inline"><em>M</em></span>.</li>
<li>Rude + Small По графику видно, что способ 4 дает наибольшее итогое качество. Таким образом итого можно порекомендовать в качестве первого гиперкуба брать rude схему, а при недостаточном качестве добавлять Small гиперкубы, полученные из матрикснета, обученного на небольшом количестве деревьев.</li>
</ol>
<div class="figure">
<img src="quality_vs_size.png" alt="качество против размера" />
<p class="caption">качество против размера</p>
</div>
<p>Стоит отметить, что при малом X матрикснет строит не самые лучшие разбиения факторов. Поэтому в дальнейших исследованиях необходимо рассмотреть способы нахождения более оптимальных разбиений, чем получается с помощью простых статистических методов.</p>
</body>
</html>
