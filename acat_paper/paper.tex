\documentclass[a4paper]{jpconf}
\usepackage{graphicx}
\begin{document}
\title{Speeding up prediction performance of the BDT-based models}

\author{
 E. Khairullin$^{1, 3}$
 and
 A. Ustyuzhanin$^{2, 3}$
}
\address{$^{1}$ Moscow Institute of Physics and Technology, Institutskiy per. 9, Dolgoprudny, Moscow Region, 141700, Russia}
\address{$^{2}$ National Research University Higher School of Economics, 20 Myasnitskaya st., Moscow 101000, Russia}
\address{$^{3}$ Yandex School of Data Analysis, 11/2, Timura Frunze st., Moscow 119021, Russia}

\ead{mikari@yandex-team.ru}

\begin{abstract}
The outcome of a machine learning algorithm is a prediction model. It is quite usual such models are computationaly expensive and growth of the quality of such models leads to the deterioration in the inference speed. However it is not always tradeoff between quality vs speed. In this paper we show it is possible to speed up model by using additional memory without loosing prediction quality significantly for a novel boosted trees algorithm called CatBoost. The idea is to combine two approaches: training fewer trees and merging trees into kind of hash maps (DecisionTensors). The proposed method allows for pareto-optimal reduction of the computational complexity of the decision tree model with regard to the quality of the model. In the considered example number of lookups was decreased from 5000 to only 6 (speedup factor of 1000) while AUC score of the model was reduced by less than per mil.
\end{abstract}

\section{Introduction}
Today, Machine Learning (ML) approach is used in various fields ranging from the information retrieval in the Internet and speech recognition to optimization of the steel composition and the search for the New Physics.
The resources (memory, CPU, network etc.) are very limited to solve the task in production settings.

For the most of the ML algorithms, improving the quality of the model usually leads to a higher levels of resource consumption. For example, when using tree-based boosting algorithms, the increase of the number of trees from tens to thousands usually leads to significant increase in the quality and at the same time the model size and inference time also grow. Higher demands for the computational resources of the more accurate models are usually solved by various technical optimizations model storage mechanism. For example vectorisation of the tree weights storage can be used. Another approach is the modification of the training process that makes it possible to obtain models with certain properties that will provide acceleration to their application without maintaining the required level of quality.

For example, Topological Trigger in LHCb is one of the places with very strict applying time restrictions. In [2] original MatrixNet [1] model allowed to significantly improve quality. But to get acceptable applying time the model was coarsed and converted to Bonsai Boosted Decision Tree format [4].

In this paper, we consider a modification of the training process of a predictive models by successor of MatrixNet, decision tree-based algorithm CatBoost [3]. But our approach is also applicable to other boosting decision tree-based algorithms.

\section{CatBoost}

CatBoost is the implementation of a tree-based gradient boosting in which oblivious decision trees are used [3]. The values of each feature $i$ are discretized into baskets using the boundaries $B_i={b_{i1},...,b_{ip_i}}$, where $p_i$ is the number of boundaries. The boundaries themselves are determined in advance using various approaches.
Original feature vector with real values is replaced by a vector of binary values $0$ and $1$ ($false$ and $true$). The value of the $i$-th attribute $f_i$ is replaced by a binary vector $G_i={g_{i1},...,g_{ip_i}}$ where $g_{ij} = f_i > b_{ij}$. Finally, all $G_i$ are concatenated into one vector. Consecutively, the resuling decision will a make bunch of boolean checks instead of comparing some real feature value with the thresholds.

Number of baskets used for the discretization could be controlled by specifying the appropriate parameter during the training procedure.

Let's consider another important training characteristic, namely the learning rate. The smaller it is, the smaller the contribution of each next tree in the ensemble. However, this has to be compensated by a large number of trees and vice versa. The learning rate can be chosen by the cross-validation process given the fixed number of trees. Usually, the more trees the higher the model quality.

With default parameters CatBoost shows already good enough quality, so we variate only number of trees (and learning rate to compensate) because number of trees affects time. For other parameters was choosen default values.

\section{Decision Tensor}
One of the simplest ways to accelerate computational performance is the preliminary calculation. The feature vector obtained after the discretization takes a limited number of values. Therefore, we can precompute the predictions for all possible values combination.

Let $n$ be the number of features. Let us consider some model $Z$, trained by some algorithm, containing a set of trees $T$. For convenience, we assume that the values of every feature belongs to the interval $[0;1]$. In other words, every feature vectors lies in n-dimensional cube $[0;1]$.

Let $B_i$ be the a of basket boundaries for the $i$-th feature, which was used in $T$. Then we can split the hyper-cube $[0;1]$ into small hyper-parallelepipeds. The prediction of the model is constant within each hyperparallelepiped. In fact, the model $Z$ allows you to calculate the value at any point of our hypercube. if we pre-calculate the value for each piece, then we will accelerate the application of our model, effectively reducing it to the time required to search a piece that contains the point. We called such approach a Decision Tensor. The asymptotic complexity of the application of the tree-based mode it $\mathcal(h\cdot t)$ where $h$ is the height of the trees, $t$ is the total number of the trees. Asymptotic complexity of Tensor is $\mathcal(n\cdot \log(b))$ where $n$ is the number of the features and $b$ is the number of cuts per feature (this complexity is estimated by using binary search). It should be noted for small $b$ simple linear search could be a bit faster that binary. So, with a small number of features the Tensor-based prediction may be computed much faster than tree-based model.

However, Decision Tensor has larger memory footprint. It needs $O(b^n)$ bytes of memory and it may be too large for common computer. Therefore, the precomputed tensor can only be used for small $b$ and $n$.

In the general, the size of one tensor $D$ can be estimated as follows:

$$D = (B_{1}, ..., B_{n}),$$

$$B_{i} = \{b_{i1}, ..., b_{ip_{i}}\},$$

$$S(D) \propto \prod |B_{i}| = \prod p_i$$

Consider the case for small $n$. We can get a small $b$ in two main ways: limit the number of cuts for each feature and train the model with big enough tree count or severely limit the tree count, then the total number of effective cuts will be exactly $h\cdot t$ without taking into account repeated use of the same cuts.

The first method was used in [2]: there a trained MatrixNet (ancestor of CatBoost) model with a small number of cuts that has been precomputed into a Decision Tensor.

The consumption of memory for a single tensor can be extremely huge provided that acceptable quality is achieved, so the natural extension of the idea is the construction of Decision Tensors ensemble but with some deterioration of applying time.

\section{Tensor Similarity}
Let's introduce the metric for the similarity of two tensors. Suppose, we have two tensors: $D^1 = (B^{1}_{1}, ..., B^{1}_{n})$ and $D^2 = (B^{2}_{1}, ..., B^{2}_{n})$. The sum $D^u = D^1 + D^2$ could be defined as $D^u = (f^{u}_{1}, ..., f^{u}_{f})$, where $f^{u}_{i} = f^{1}_{i} \cup f^{2}_{i}$. Now, let's define similarity as the difference between the sum of their sizes and the size of their union:

$$Sim(D^1, D^2) = S(D^1) + S(D^2) - S(D^1 + D^2)$$

% \section{Multicriteria optimization}
% (TODO - probably should be removed)
% The target metrics are $ Q $ (model quality), $ S $ (size), and $ t $ (time of inference). However, we can assume that the inference time is proportional to the number of decision tensors $ M $.
% The input feature vectors will define the algorithm for obtaining the trees and the algorithm for combining them into the Tensors. We denote single vector by $ v $ and let $V$ be the set of all input vectors. Then we need to solve the following problem:

% $$min(Q(V), S(V), M(V)), v \in V$$

% It should be noted that for practical application this problem can be solved using the constrained optimization method. But in practice there are quite strict limitations on the size of the model and the available application time (TODO - what did you mean?).

\section{Algorithm for combining set of Decision Trees to a set of Decision Tensors}

Suppose we have $T$ trees and we want to combine them in $M$ Decision Tensors. The efficiency metric of the partition is the result Tensor set memory footprint.
In general, this is not an easy task and it requires a separate study. We used the following heuristic approach:
\medskip
\renewcommand{\theenumi}{\arabic{enumi}}
\begin{enumerate}
\item Initialize $M$ Tensors by placing in each one an arbitrarily chosen tree
\item Choose any of the remaining trees
\item Combine it with the most similar tensor
\item Repeat steps 2-3 until there are no unused trees left.
\end{enumerate}
Such reduction of the set of trees to a set of Tensors we call partitioning.

\section{A complete algorithm for constructing an Decision Tensors Ensemble}

There are two key approaches:

\medskip
\begin{itemize}
\item We fix a very rough (with very few ) partition by a small number of Tensors and train an arbitrarily large number of trees.
\item The partitioning is done quite detailed, but at the same time we significantly reduce the number of trained trees, thereby reducing the number of actually used buckets.
\end{itemize}

The algorithm actively uses both of these ideas.

\medskip
\renewcommand{\theenumi}{\arabic{enumi}}
\begin{enumerate}
\item Determine time and size constraints
\item Determine the desired number of Tensors (M) based on time constraints
\item Train the model with a limited number of buckets
\item Merge the trees from the obtained model into the first Decision Tensor from the ensemble
\item Choose another set of hyperparameters using random search
\medskip
\begin{itemize}
\item Train the model, using the previously trained model as the baseline
\item Transform it into M-1 Decision Tensors
\end{itemize}
\item Remove ensembles that do not fit the required size limits
\item Choose the ensemble with the best quality on an additional data sample.
\end{enumerate}

This algorithm allows to find the Pareto-optimal ensemble of Decision Tensors.

\section{Real problem illustration}

We have used CatBoost as a main algorithm and also took the dataset and the model from [2]. Number of features in the dataset is 10, the model was trained with 5000 trees and 64 buckets. We compared our algorithm for constructing Decision Tensors Ensemble with trivial approach: training a small number of trees but with high value of the training speed.
For this we divided the data set into three parts: training (50\%), test (25\%) and validation (25\%). We trained the models on the training sample, then used the test for the selection of the best models under given constraints.

We have limited the number of tensor by 6 and the total size of models by one gigabyte, so every prediction requires 4 bytes (float). We have scanned only through the key CatBoost hyperparameters: number of trees and the learing rate.

The figure of merit we've taken also from [2], which is the area under the ROC curve where False Positive Rate less than 0.05. For the convenience, we have normalized this metric so that its maximum value (with an ideal model) is equal to 1.

\section{Discussion}

We compared the CatBoost model with 5000 trees, the fast CatBoost models for 1-125 trees and the Decision Tensors Ensembles (DTE) with different memory limits and different tensors numbers.

Figure 1 shows the dependence of the quality on the number of lookups. Figure 2 shows the dependence of the quality vs the total inference time at the full validation dataset. Table 1 shows properties of the models trained in the process.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.8\textwidth]{"lookups.png"}
\caption{Lookups count versus FOM for different models sizes}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.8\textwidth]{"times.png"}
\caption{Application time versus FOM for different models sizes}
\end{center}
\end{figure}

\begin{table}[h]
\caption{\label{tabone}Properties of the some models trained in the process}

\begin{center}
\lineup
\begin{tabular}{*{4}{l}}
\br
Model & Roc & Time, ms & Size, Mb \cr
\mr
catboost-5000 & $0.6004$ & $10000$ & $<5$ \cr
catboost-100 & $0.589$ & $160$ & $<5$ \cr
catboost-75 & $0.5808$ & $125$ & $<5$ \cr
catboost-10 & $0.5336$ & $30$ & $<5$ \cr
big dte-6 & $0.5952$ & $180$ & $1000$ \cr
big dte-3 & $0.5938$ & $145$ & $1000$ \cr
big dte-2 & $0.5844$ & $70$ & $1000$ \cr
small dte-2 & $0.5794$ & $55$ & $150$ \cr
dte-1 & $0.5618$ & $30$ & $150$ \cr
\br
\end{tabular}
\end{center}
\end{table}

It can be seen that at a comparable quality with fast CatBoost models (1-125 trees), DTE allows a 25 times reduction in the number of lookups. And in comparison with the best model (5000 trees), the number of lookups is 1000 times smaller with a small loss in quality ($ <0.006 $ with the best quality of 0.6).

However, the difference in working time is not so significant - 2.5 times and 50 times, respectively. This is due to the principles of the CPU caching and the size of our dataset. Since the Decision Tensor occupies significant amount of memory and does not fit into the CPU cache almost every lookup results in (slow) reading from the RAM. CatBoost models are small enough to fit into the cache. There was a single operation type of model application in our tests, so the CatBoost models stayed persistently in the cache. In real applications, CatBoost model will most likely not stay in the cache alone and will be even slower because of this. On the contrary the performance of the Decision Tensor Ensemble will not degrade due to that fact.

\section{Conclusion}

Decision Tensor Ensbles allow you to speed up the inference by complex Boosting Decision Trees algorithms (such as CatBoost) in real-time systems. Considered algorithm for building Tensor ensembles allows for additional tradeoff between memory and the inference speed without significant loss of the model quality.
In addition, the use of the Ensemble can significantly improve the model quality in comparison with a single Decision Tensor and can be used in the triggers.


\section*{References}
\numrefs{99}
\item Gulin A, Kuralenok I, Pavlov D 2011 Winning The Transfer Learning Track of Yahoo!'s Learning To Rank Challenge with YetiRank {\it Yahoo! Learning to Rank Challenge} p 63--76
\item Likhomanenko T, Ilten P, Khairullin E, Rogozhnikov A, Ustyuzhanin A, Williams M 2015 LHCb Topological Trigger Reoptimization (IOP Publishing) {\it Journal of Physics: Conference Series} Vol. 664, No. 8, p. 082025
\item Dorogush A, Gulin A Gusev G Kazeev N Ostroumova L Vorobev A Fighting biases with dynamic boosting arXiv:1706.09516
\item Gligorov V, Williams M 2013 Efficient, reliable and fast high-level triggering using a bonsai boosted decision tree {\it Journal of Instrumentation} 8 P02013
\endnumrefs

\end{document}


